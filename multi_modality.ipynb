{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9JFdIRkxhmn",
        "outputId": "cbb7df06-90d2-4462-d656-05279706f7bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-{torch._version_}.html\n",
            "Collecting torch-scatter\n",
            "  Using cached torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-sparse\n",
            "  Using cached torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-cluster\n",
            "  Using cached torch_cluster-1.6.3.tar.gz (54 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-spline-conv\n",
            "  Using cached torch_spline_conv-1.2.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.14.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: torch-scatter, torch-sparse, torch-cluster, torch-spline-conv\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=547368 sha256=4aff1fb36338f9c05709fa9b47019ae895453984faa23336d057ff61b720c03e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.18-cp311-cp311-linux_x86_64.whl size=1127937 sha256=3744221985866b95082465303fa6034bec01822713dda28a777fc2ee4c8b44ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/e2/1e/299c596063839303657c211f587f05591891cc6cf126d94d21\n",
            "  Building wheel for torch-cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-cluster: filename=torch_cluster-1.6.3-cp311-cp311-linux_x86_64.whl size=739064 sha256=971cd6a79f92780aaa2f6af888d8e5779b10d538a41f24729ed004e0f118cc9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/de/7d/a4211822af99147b93800e9e204f0be21294e3c0b95b3b861a\n",
            "  Building wheel for torch-spline-conv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-spline-conv: filename=torch_spline_conv-1.2.2-cp311-cp311-linux_x86_64.whl size=228695 sha256=4ad9173004e08c5e82af55743d83b1f3a27ca04431e8edfcf72b3a0afb0f1ff6\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/16/8a/a98b0173c4fbbc7aa1c4929b46d2eb08d1475c5c7b54e289b6\n",
            "Successfully built torch-scatter torch-sparse torch-cluster torch-spline-conv\n",
            "Installing collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster, torch-geometric\n",
            "Successfully installed torch-cluster-1.6.3 torch-geometric-2.6.1 torch-scatter-2.1.2 torch-sparse-0.6.18 torch-spline-conv-1.2.2\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'torch_geometric' has no attribute '_version_'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cf71c89d6da7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-{torch._version_}.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch_geometric version:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHeteroData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAGEConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHeteroConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGATConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch_geometric' has no attribute '_version_'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-{torch._version_}.html\n",
        "import torch_geometric\n",
        "print(\"torch_geometric version:\", torch_geometric._version_)\n",
        "from torch_geometric.data import Data, HeteroData\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, HeteroConv, GATConv, GAT\n",
        "from torch_geometric.nn import DataLoader, NeighborLoader\n",
        "from torch_geometric.utils import to_networkx, to_dense_adj\n",
        "import networkx as nx\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from transformers import CLIPModel, CLIPProcessor, BertModel, BertTokenizer\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import copy\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "\n",
        "class KGBenchExplorer:\n",
        "    def _init_(self, data_path):\n",
        "        \"\"\"\n",
        "        Initialize the explorer with the data path\n",
        "\n",
        "        Args:\n",
        "            data_path: Path to the dmg777k dataset folder\n",
        "        \"\"\"\n",
        "        self.data_path = data_path\n",
        "        self.train_data = None\n",
        "        self.valid_data = None\n",
        "        self.test_data = None\n",
        "        self.entity_types = None\n",
        "        self.relation_types = None\n",
        "        self.node_features = None\n",
        "        self.e2i = None\n",
        "        self.i2e = None\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load the dataset files\"\"\"\n",
        "        with open(os.path.join(self.data_path, 'train.json'), 'r') as f:\n",
        "            self.train_data = json.load(f)\n",
        "        with open(os.path.join(self.data_path, 'valid.json'), 'r') as f:\n",
        "            self.valid_data = json.load(f)\n",
        "        with open(os.path.join(self.data_path, 'test.json'), 'r') as f:\n",
        "            self.test_data = json.load(f)\n",
        "        self.e2i = self.train_data.get('e2i', {})\n",
        "        self.i2e = self.train_data.get('i2e', {})\n",
        "        print(f\"Loaded dataset with {len(self.e2i)} entities\")\n",
        "        self._extract_entity_types()\n",
        "        return True\n",
        "\n",
        "    def _extract_entity_types(self):\n",
        "        \"\"\"Extract entity types from URIs or other identifiers\"\"\"\n",
        "        self.entity_type_map = {}\n",
        "        for entity, entity_id in self.e2i.items():\n",
        "            if isinstance(entity, str):\n",
        "                parts = entity.split('/')\n",
        "                if len(parts) > 2:\n",
        "                    entity_type = parts[-2]\n",
        "                elif '#' in entity:\n",
        "                    entity_type = entity.split('#')[-2]\n",
        "                else:\n",
        "                    entity_type = 'unknown'\n",
        "            else:\n",
        "                entity_type = 'unknown'\n",
        "            self.entity_type_map[entity_id] = entity_type\n",
        "\n",
        "    def analyze_entity_types(self):\n",
        "        \"\"\"Analyze and plot the types and number of entities (nodes)\"\"\"\n",
        "        if not self.train_data:\n",
        "            self.load_data()\n",
        "        entity_types = {}\n",
        "        for entity, entity_id in self.e2i.items():\n",
        "            if isinstance(entity, str):\n",
        "                if entity.startswith('http://'):\n",
        "                    parts = entity.split('/')\n",
        "                    if len(parts) > 2:\n",
        "                        entity_type = parts[-2]\n",
        "                    else:\n",
        "                        entity_type = 'unknown'\n",
        "                elif '#' in entity:\n",
        "                    entity_type = entity.split('#')[-2]\n",
        "                else:\n",
        "                    entity_type = 'unknown'\n",
        "            else:\n",
        "                entity_type = 'unknown'\n",
        "            entity_types[entity_type] = entity_types.get(entity_type, 0) + 1\n",
        "        self.entity_types = entity_types\n",
        "\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        sorted_types = sorted(entity_types.items(), key=lambda x: x[1], reverse=True)\n",
        "        labels = [t[0] for t in sorted_types]\n",
        "        values = [t[1] for t in sorted_types]\n",
        "        if len(labels) > 15:\n",
        "            labels = labels[:14] + ['Others']\n",
        "            values = values[:14] + [sum(values[14:])]\n",
        "        bars = plt.bar(labels, values, color=plt.cm.viridis(np.linspace(0, 1, len(labels))))\n",
        "        plt.title('Distribution of Entity Types', fontsize=16)\n",
        "        plt.xlabel('Entity Type', fontsize=12)\n",
        "        plt.ylabel('Count', fontsize=12)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.1, f'{int(height):,}', ha='center', va='bottom', fontsize=9)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('entity_types_distribution.png', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        if len(labels) > 7:\n",
        "            pie_labels = labels[:6] + ['Others']\n",
        "            pie_values = values[:6] + [sum(values[6:])]\n",
        "        else:\n",
        "            pie_labels = labels\n",
        "            pie_values = values\n",
        "        plt.pie(pie_values, labels=pie_labels, autopct='%1.1f%%', startangle=140, colors=plt.cm.tab20(np.linspace(0, 1, len(pie_labels))))\n",
        "        plt.axis('equal')\n",
        "        plt.title('Entity Types Distribution', fontsize=16)\n",
        "        plt.savefig('entity_types_pie.png', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        print(\"\\n=== Entity Types Analysis ===\")\n",
        "        print(f\"Total unique entity types: {len(entity_types)}\")\n",
        "        print(\"Top 5 entity types:\")\n",
        "        for i, (type_name, count) in enumerate(sorted_types[:5]):\n",
        "            print(f\"{i+1}. {type_name}: {count:,} entities ({count/sum(values)*100:.1f}%)\")\n",
        "        return entity_types\n",
        "\n",
        "    def analyze_edge_types(self):\n",
        "        \"\"\"Analyze and plot the types and number of edges\"\"\"\n",
        "        if not self.train_data:\n",
        "            self.load_data()\n",
        "        edge_types = {}\n",
        "        for s, p, o in self.train_data.get('triples', []):\n",
        "            edge_types[p] = edge_types.get(p, 0) + 1\n",
        "        self.relation_types = edge_types\n",
        "\n",
        "        plt.figure(figsize=(16, 10))\n",
        "        sorted_types = sorted(edge_types.items(), key=lambda x: x[1], reverse=True)\n",
        "        labels = [t[0] for t in sorted_types]\n",
        "        values = [t[1] for t in sorted_types]\n",
        "        if len(labels) > 15:\n",
        "            labels = labels[:14] + ['Others']\n",
        "            values = values[:14] + [sum(values[14:])]\n",
        "        bars = plt.bar(labels, values, color=plt.cm.cool(np.linspace(0, 1, len(labels))))\n",
        "        plt.title('Distribution of Edge Types', fontsize=16)\n",
        "        plt.xlabel('Edge Type', fontsize=12)\n",
        "        plt.ylabel('Count', fontsize=12)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.1, f'{int(height):,}', ha='center', va='bottom', fontsize=9)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('edge_types_distribution.png', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        if len(labels) > 7:\n",
        "            pie_labels = labels[:6] + ['Others']\n",
        "            pie_values = values[:6] + [sum(values[6:])]\n",
        "        else:\n",
        "            pie_labels = labels\n",
        "            pie_values = values\n",
        "        plt.pie(pie_values, labels=pie_labels, autopct='%1.1f%%', startangle=140, colors=plt.cm.tab20(np.linspace(0, 1, len(pie_labels))))\n",
        "        plt.axis('equal')\n",
        "        plt.title('Edge Types Distribution', fontsize=16)\n",
        "        plt.savefig('edge_types_pie.png', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        print(\"\\n=== Edge Types Analysis ===\")\n",
        "        print(f\"Total unique edge types: {len(edge_types)}\")\n",
        "        print(\"Top 5 edge types:\")\n",
        "        for i, (type_name, count) in enumerate(sorted_types[:5]):\n",
        "            print(f\"{i+1}. {type_name}: {count:,} edges ({count/sum(values)*100:.1f}%)\")\n",
        "        return edge_types\n",
        "\n",
        "    def analyze_node_features(self):\n",
        "        \"\"\"Analyze what type of information is present per node\"\"\"\n",
        "        if not self.train_data:\n",
        "            self.load_data()\n",
        "        text_features = {entity_id: len(text.split()) for entity_id, text in self.train_data.get('texts', {}).items()}\n",
        "        image_features = {entity_id: 1 for entity_id in self.train_data.get('images', {}).keys()}\n",
        "\n",
        "        if text_features:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            text_lengths = list(text_features.values())\n",
        "            plt.hist(text_lengths, bins=50, alpha=0.75, color='skyblue', edgecolor='black')\n",
        "            plt.title('Distribution of Text Feature Lengths', fontsize=16)\n",
        "            plt.xlabel('Text Length (words)', fontsize=12)\n",
        "            plt.ylabel('Frequency', fontsize=12)\n",
        "            plt.grid(alpha=0.3)\n",
        "            stats_text = f\"Min: {min(text_lengths)}\\nMax: {max(text_lengths)}\\nMean: {np.mean(text_lengths):.1f}\\nMedian: {np.median(text_lengths):.1f}\"\n",
        "            plt.annotate(stats_text, xy=(0.75, 0.75), xycoords='axes fraction', bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", alpha=0.8))\n",
        "            plt.savefig('text_length_distribution.png', dpi=300)\n",
        "            plt.close()\n",
        "\n",
        "        if text_features or image_features:\n",
        "            plt.figure(figsize=(10, 10))\n",
        "            text_ids = set(text_features.keys())\n",
        "            image_ids = set(image_features.keys())\n",
        "            text_only = len(text_ids - image_ids)\n",
        "            image_only = len(image_ids - text_ids)\n",
        "            both = len(text_ids.intersection(image_ids))\n",
        "            plt.bar(['Text Only', 'Image Only', 'Both'], [text_only, image_only, both], color=['skyblue', 'lightgreen', 'salmon'])\n",
        "            plt.title('Distribution of Feature Modalities', fontsize=16)\n",
        "            plt.ylabel('Number of Nodes', fontsize=12)\n",
        "            for i, v in enumerate([text_only, image_only, both]):\n",
        "                plt.text(i, v + 0.1, f\"{v:,}\", ha='center')\n",
        "            plt.savefig('modality_distribution.png', dpi=300)\n",
        "            plt.close()\n",
        "\n",
        "        self.node_features = {'text': text_features, 'image': image_features}\n",
        "        print(\"\\n=== Node Features Analysis ===\")\n",
        "        print(f\"Nodes with text features: {len(text_features):,}\")\n",
        "        print(f\"Nodes with image features: {len(image_features):,}\")\n",
        "        print(f\"Nodes with both text and image: {len(set(text_features.keys()).intersection(set(image_features.keys()))):,}\")\n",
        "        if text_features:\n",
        "            print(f\"\\nText feature statistics:\")\n",
        "            print(f\"  Min length: {min(text_features.values())} words\")\n",
        "            print(f\"  Max length: {max(text_features.values())} words\")\n",
        "            print(f\"  Mean length: {np.mean(list(text_features.values())):.1f} words\")\n",
        "            print(f\"  Median length: {np.median(list(text_features.values())):.1f} words\")\n",
        "        return self.node_features\n",
        "\n",
        "    def analyze_label_distribution(self):\n",
        "        \"\"\"Analyze the distribution of node labels\"\"\"\n",
        "        if not self.train_data:\n",
        "            self.load_data()\n",
        "        labels = self.train_data.get('labels', {})\n",
        "        if not labels:\n",
        "            print(\"No labels found in the dataset.\")\n",
        "            return None\n",
        "        label_counts = Counter(labels.values())\n",
        "\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        sorted_labels = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "        label_names = [l[0] for l in sorted_labels]\n",
        "        label_values = [l[1] for l in sorted_labels]\n",
        "        if len(label_names) > 20:\n",
        "            label_names = label_names[:19] + ['Others']\n",
        "            label_values = label_values[:19] + [sum(label_values[19:])]\n",
        "        bars = plt.bar(label_names, label_values, color=plt.cm.plasma(np.linspace(0, 1, len(label_names))))\n",
        "        plt.title('Distribution of Node Labels', fontsize=16)\n",
        "        plt.xlabel('Label', fontsize=12)\n",
        "        plt.ylabel('Count', fontsize=12)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.1, f'{int(height):,}', ha='center', va='bottom', fontsize=9)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('label_distribution.png', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        print(\"\\n=== Label Distribution Analysis ===\")\n",
        "        print(f\"Total labeled nodes: {sum(label_counts.values()):,}\")\n",
        "        print(f\"Number of unique labels: {len(label_counts)}\")\n",
        "        print(\"Top 5 most common labels:\")\n",
        "        for i, (label, count) in enumerate(sorted_labels[:5]):\n",
        "            print(f\"{i+1}. {label}: {count:,} nodes ({count/sum(label_values)*100:.1f}%)\")\n",
        "        imbalance_ratio = max(label_values) / min(label_values)\n",
        "        print(f\"\\nClass imbalance ratio (max/min): {imbalance_ratio:.2f}\")\n",
        "        return label_counts\n",
        "\n",
        "    def visualize_graph_sample(self, max_nodes=150):\n",
        "        \"\"\"Visualize a small sample of the graph with node types\"\"\"\n",
        "        if not self.train_data:\n",
        "            self.load_data()\n",
        "        G = nx.DiGraph()\n",
        "        triples = self.train_data.get('triples', [])\n",
        "        sampled_triples = []\n",
        "        if triples:\n",
        "            start_triple = random.choice(triples)\n",
        "            sampled_triples.append(start_triple)\n",
        "            included_nodes = {start_triple[0], start_triple[2]}\n",
        "            candidates = [t for t in triples if t[0] in included_nodes or t[2] in included_nodes]\n",
        "            while len(included_nodes) < max_nodes and candidates:\n",
        "                new_triple = random.choice(candidates)\n",
        "                if new_triple not in sampled_triples:\n",
        "                    sampled_triples.append(new_triple)\n",
        "                    included_nodes.add(new_triple[0])\n",
        "                    included_nodes.add(new_triple[2])\n",
        "                candidates = [t for t in triples if (t[0] in included_nodes or t[2] in included_nodes) and t not in sampled_triples]\n",
        "                if len(candidates) > 1000:\n",
        "                    candidates = random.sample(candidates, 1000)\n",
        "        if len(sampled_triples) < min(50, len(triples)):\n",
        "            sampled_triples = random.sample(triples, min(len(triples), max_nodes * 2))\n",
        "\n",
        "        node_modalities = {}\n",
        "        for s, p, o in sampled_triples:\n",
        "            G.add_edge(s, o, relation=p)\n",
        "            for node in [s, o]:\n",
        "                if str(node) not in node_modalities:\n",
        "                    node_modalities[str(node)] = []\n",
        "                if str(node) in self.train_data.get('texts', {}):\n",
        "                    if 'text' not in node_modalities[str(node)]:\n",
        "                        node_modalities[str(node)].append('text')\n",
        "                if str(node) in self.train_data.get('images', {}):\n",
        "                    if 'image' not in node_modalities[str(node)]:\n",
        "                        node_modalities[str(node)].append('image')\n",
        "\n",
        "        node_colors = []\n",
        "        for node in G.nodes():\n",
        "            modalities = node_modalities.get(str(node), [])\n",
        "            if 'text' in modalities and 'image' in modalities:\n",
        "                node_colors.append('purple')\n",
        "            elif 'text' in modalities:\n",
        "                node_colors.append('blue')\n",
        "            elif 'image' in modalities:\n",
        "                node_colors.append('green')\n",
        "            else:\n",
        "                node_colors.append('gray')\n",
        "\n",
        "        node_labels = {node: self.i2e.get(str(node), str(node)).split('/')[-1] if len(G.nodes()) <= 50 else '' for node in G.nodes()}\n",
        "\n",
        "        plt.figure(figsize=(14, 14))\n",
        "        pos = nx.spring_layout(G, seed=42)\n",
        "        nx.draw_networkx_nodes(G, pos, node_size=100, node_color=node_colors, alpha=0.8)\n",
        "        nx.draw_networkx_edges(G, pos, width=0.5, alpha=0.5, arrows=True, arrowsize=10)\n",
        "        if len(G.nodes()) <= 50:\n",
        "            nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=8)\n",
        "        legend_elements = [\n",
        "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='purple', markersize=10, label='Text & Image'),\n",
        "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Text Only'),\n",
        "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='Image Only'),\n",
        "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', markersize=10, label='No Features')\n",
        "        ]\n",
        "        plt.legend(handles=legend_elements, loc='upper right')\n",
        "        plt.title(f'Sample of Graph Structure ({len(G.nodes())} nodes, {len(G.edges())} edges)', fontsize=16)\n",
        "        plt.axis('off')\n",
        "        plt.savefig('graph_sample.png', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        print(\"\\n=== Graph Sample Visualization ===\")\n",
        "        print(f\"Sample contains {len(G.nodes())} nodes and {len(G.edges())} edges\")\n",
        "        modality_counts = Counter([tuple(node_modalities.get(str(node), [])) for node in G.nodes()])\n",
        "        print(\"Node modality distribution in sample:\")\n",
        "        for modality_tuple, count in modality_counts.items():\n",
        "            modality_str = ', '.join(modality_tuple) if modality_tuple else 'No features'\n",
        "            print(f\"  {modality_str}: {count} nodes\")\n",
        "        avg_degree = sum(dict(G.degree()).values()) / len(G.nodes())\n",
        "        print(f\"Average node degree: {avg_degree:.2f}\")\n",
        "        try:\n",
        "            density = nx.density(G)\n",
        "            print(f\"Graph density: {density:.6f}\")\n",
        "        except:\n",
        "            print(\"Could not calculate graph density.\")\n",
        "        connected_components = list(nx.weakly_connected_components(G))\n",
        "        print(f\"Number of weakly connected components: {len(connected_components)}\")\n",
        "        return G\n",
        "\n",
        "    def analyze_triples(self):\n",
        "        \"\"\"Analyze how the triples attribute defines graphs\"\"\"\n",
        "        if not self.train_data:\n",
        "            self.load_data()\n",
        "        triples = self.train_data.get('triples', [])\n",
        "        subjects = set([s for s, _, _ in triples])\n",
        "        predicates = set([p for _, p, _ in triples])\n",
        "        objects = set([o for _, _, o in triples])\n",
        "        subject_degrees = Counter([s for s, _, _ in triples])\n",
        "        object_degrees = Counter([o for _, _, o in triples])\n",
        "        node_degrees = Counter(subject_degrees) + Counter(object_degrees)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        degrees = list(node_degrees.values())\n",
        "        plt.hist(degrees, bins=50, alpha=0.75, color='teal', log=True, edgecolor='black')\n",
        "        plt.title('Node Degree Distribution (Log Scale)', fontsize=16)\n",
        "        plt.xlabel('Node Degree', fontsize=12)\n",
        "        plt.ylabel('Frequency (Log Scale)', fontsize=12)\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.savefig('node_degree_distribution.png', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        print(\"\\n=== Triple Analysis ===\")\n",
        "        print(f\"Total triples: {len(triples):,}\")\n",
        "        print(f\"Unique subjects: {len(subjects):,}\")\n",
        "        print(f\"Unique predicates: {len(predicates):,}\")\n",
        "        print(f\"Unique objects: {len(objects):,}\")\n",
        "        predicate_counts = Counter([p for _, p, _ in triples])\n",
        "        print(\"\\nTop 5 most common predicates:\")\n",
        "        for i, (pred, count) in enumerate(predicate_counts.most_common(5)):\n",
        "            print(f\"{i+1}. {pred}: {count:,} occurrences\")\n",
        "        print(\"\\nNode degree statistics:\")\n",
        "        print(f\"  Minimum degree: {min(degrees)}\")\n",
        "        print(f\"  Maximum degree: {max(degrees)}\")\n",
        "        print(f\"  Average degree: {np.mean(degrees):.2f}\")\n",
        "        print(f\"  Median degree: {np.median(degrees)}\")\n",
        "        print(\"\\nSample triples (subject, predicate, object):\")\n",
        "        for s, p, o in random.sample(triples, min(5, len(triples))):\n",
        "            s_entity = self.i2e.get(str(s), f\"Entity {s}\")\n",
        "            o_entity = self.i2e.get(str(o), f\"Entity {o}\")\n",
        "            print(f\"  {s_entity} --[{p}]--> {o_entity}\")\n",
        "        return {\n",
        "            'subjects': len(subjects),\n",
        "            'predicates': len(predicates),\n",
        "            'objects': len(objects),\n",
        "            'triples': len(triples),\n",
        "            'degree_stats': {'min': min(degrees), 'max': max(degrees), 'mean': np.mean(degrees), 'median': np.median(degrees)}\n",
        "        }\n",
        "\n",
        "    def analyze_e2i_i2e(self):\n",
        "        \"\"\"Analyze the e2i and i2e mappings\"\"\"\n",
        "        if not self.train_data:\n",
        "            self.load_data()\n",
        "        e2i = self.train_data.get('e2i', {})\n",
        "        i2e = self.train_data.get('i2e', {})\n",
        "        print(\"\\n=== Entity to ID Mapping Analysis ===\")\n",
        "        print(f\"Total mappings: {len(e2i):,}\")\n",
        "        uri_patterns = Counter('/'.join(entity.split('/')[:3]) if '/' in entity else entity.split('#')[0] if '#' in entity else 'other' for entity in e2i.keys() if isinstance(entity, str))\n",
        "        print(\"\\nMost common URI patterns:\")\n",
        "        for i, (pattern, count) in enumerate(uri_patterns.most_common(5)):\n",
        "            print(f\"{i+1}. {pattern}: {count:,} entities\")\n",
        "        print(\"\\nSample e2i mappings:\")\n",
        "        samples = random.sample(list(e2i.items()), min(5, len(e2i)))\n",
        "        for entity, entity_id in samples:\n",
        "            print(f\"  {entity} -> {entity_id}\")\n",
        "        return {'e2i_count': len(e2i), 'i2e_count': len(i2e), 'uri_patterns': dict(uri_patterns)}\n",
        "\n",
        "    def run_complete_eda(self):\n",
        "        \"\"\"Run all EDA functions and print a summary\"\"\"\n",
        "        print(\"=== Starting Exploratory Data Analysis of dmg777k Dataset ===\")\n",
        "        self.load_data()\n",
        "        entity_types = self.analyze_entity_types()\n",
        "        edge_types = self.analyze_edge_types()\n",
        "        node_features = self.analyze_node_features()\n",
        "        label_distribution = self.analyze_label_distribution()\n",
        "        triple_stats = self.analyze_triples()\n",
        "        e2i_stats = self.analyze_e2i_i2e()\n",
        "        graph_sample = self.visualize_graph_sample()\n",
        "\n",
        "        with open('eda_report.txt', 'w') as f:\n",
        "            f.write(\"=== dmg777k Dataset Analysis Report ===\\n\\n\")\n",
        "            f.write(f\"Total entities: {len(self.e2i):,}\\n\")\n",
        "            f.write(f\"Entity types: {len(entity_types)}\\n\")\n",
        "            f.write(f\"Edge types: {len(edge_types)}\\n\")\n",
        "            f.write(f\"Nodes with text: {len(node_features['text']):,}\\n\")\n",
        "            f.write(f\"Nodes with images: {len(node_features['image']):,}\\n\")\n",
        "            f.write(f\"Total triples: {triple_stats['triples']:,}\\n\")\n",
        "            f.write(\"\\nNode degree statistics:\\n\")\n",
        "            for key, value in triple_stats['degree_stats'].items():\n",
        "                f.write(f\"  {key}: {value}\\n\")\n",
        "            if label_distribution:\n",
        "                f.write(f\"\\nNumber of labeled nodes: {sum(label_distribution.values()):,}\\n\")\n",
        "                f.write(f\"Number of unique labels: {len(label_distribution)}\\n\")\n",
        "        print(\"\\n=== EDA Report ===\")\n",
        "        print(\"EDA report saved to 'eda_report.txt'.\")\n",
        "        return True\n",
        "\n",
        "class MultimodalGNN(nn.Module):\n",
        "    def _init_(self, num_nodes, hidden_dim, num_classes, text_embeddings=None, image_embeddings=None):\n",
        "        \"\"\"Multimodal GNN for node classification\"\"\"\n",
        "        super(MultimodalGNN, self)._init_()\n",
        "        self.node_emb = nn.Embedding(num_nodes, hidden_dim)\n",
        "        self.has_text = text_embeddings is not None\n",
        "        if self.has_text:\n",
        "            text_dim = text_embeddings.shape[1]\n",
        "            self.text_projection = nn.Linear(text_dim, hidden_dim)\n",
        "            self.register_buffer('text_embeddings', torch.from_numpy(text_embeddings).float())\n",
        "            self.text_nodes = torch.arange(len(text_embeddings))\n",
        "        self.has_image = image_embeddings is not None\n",
        "        if self.has_image:\n",
        "            img_dim = image_embeddings.shape[1]\n",
        "            self.image_projection = nn.Linear(img_dim, hidden_dim)\n",
        "            self.register_buffer('image_embeddings', torch.from_numpy(image_embeddings).float())\n",
        "            self.image_nodes = torch.arange(len(image_embeddings))\n",
        "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, edge_index):\n",
        "        \"\"\"Forward pass through the GNN\"\"\"\n",
        "        x = self.node_emb.weight\n",
        "        if self.has_text:\n",
        "            text_proj = self.text_projection(self.text_embeddings)\n",
        "            x[self.text_nodes] = x[self.text_nodes] + text_proj\n",
        "        if self.has_image:\n",
        "            image_proj = self.image_projection(self.image_embeddings)\n",
        "            x[self.image_nodes] = x[self.image_nodes] + image_proj\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        node_logits = self.classifier(x)\n",
        "        return node_logits\n",
        "\n",
        "def extract_features(kg_explorer):\n",
        "    \"\"\"Extract features from text and images using pre-trained models\"\"\"\n",
        "    print(\"Extracting features from text and images...\")\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    bert_model = BertModel.from_pretrained('bert-base-uncased').to(kg_explorer.device)\n",
        "    bert_model.eval()\n",
        "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(kg_explorer.device)\n",
        "    clip_model.eval()\n",
        "\n",
        "    text_features = {}\n",
        "    for entity_id, text in tqdm(kg_explorer.train_data.get('texts', {}).items()):\n",
        "        if len(text) > 512:\n",
        "            text = text[:512]\n",
        "        with torch.no_grad():\n",
        "            inputs = bert_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(kg_explorer.device)\n",
        "            outputs = bert_model(**inputs)\n",
        "            text_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "            text_features[entity_id] = text_embedding[0]\n",
        "\n",
        "    image_features = {}\n",
        "    for entity_id, image_path in tqdm(kg_explorer.train_data.get('images', {}).items()):\n",
        "        try:\n",
        "            if image_path.startswith('http'):\n",
        "                response = requests.get(image_path)\n",
        "                image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "            else:\n",
        "                image = Image.open(os.path.join(kg_explorer.data_path, image_path)).convert('RGB')\n",
        "            with torch.no_grad():\n",
        "                inputs = clip_processor(images=image, return_tensors=\"pt\").to(kg_explorer.device)\n",
        "                outputs = clip_model.get_image_features(**inputs)\n",
        "                image_embedding = outputs.cpu().numpy()\n",
        "                image_features[entity_id] = image_embedding[0]\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {image_path}: {e}\")\n",
        "\n",
        "    print(f\"Extracted features for {len(text_features)} texts and {len(image_features)} images\")\n",
        "    return text_features, image_features\n",
        "\n",
        "def prepare_graph_data(kg_explorer, text_features=None, image_features=None):\n",
        "    \"\"\"Prepare graph data for GNN training\"\"\"\n",
        "    print(\"Preparing graph data for GNN...\")\n",
        "    triples = kg_explorer.train_data.get('triples', [])\n",
        "    src_nodes = [s for s, _, _ in triples]\n",
        "    dst_nodes = [o for _, _, o in triples]\n",
        "    edge_index = torch.tensor([src_nodes, dst_nodes], dtype=torch.long)\n",
        "\n",
        "    labels_dict = kg_explorer.train_data.get('labels', {})\n",
        "    unique_labels = sorted(set(labels_dict.values()))\n",
        "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    num_nodes = len(kg_explorer.e2i)\n",
        "    labels = torch.zeros(num_nodes, dtype=torch.long)\n",
        "    for node_id, label in labels_dict.items():\n",
        "        labels[int(node_id)] = label_to_idx[label]\n",
        "\n",
        "    labeled_nodes = list(labels_dict.keys())\n",
        "    if kg_explorer.valid_data and kg_explorer.test_data:\n",
        "        train_idx = torch.tensor([int(idx) for idx in kg_explorer.train_data.get('nodes', [])])\n",
        "        valid_idx = torch.tensor([int(idx) for idx in kg_explorer.valid_data.get('nodes', [])])\n",
        "        test_idx = torch.tensor([int(idx) for idx in kg_explorer.test_data.get('nodes', [])])\n",
        "    else:\n",
        "        nodes = np.array([int(node) for node in labeled_nodes])\n",
        "        train_nodes, test_nodes = train_test_split(nodes, test_size=0.2, random_state=42)\n",
        "        train_nodes, valid_nodes = train_test_split(train_nodes, test_size=0.15, random_state=42)\n",
        "        train_idx = torch.tensor(train_nodes)\n",
        "        valid_idx = torch.tensor(valid_nodes)\n",
        "        test_idx = torch.tensor(test_nodes)\n",
        "\n",
        "    text_embeddings = None\n",
        "    if text_features:\n",
        "        text_feature_matrix = np.zeros((num_nodes, list(text_features.values())[0].shape[0]))\n",
        "        for node_id, embedding in text_features.items():\n",
        "            text_feature_matrix[int(node_id)] = embedding\n",
        "        text_embeddings = text_feature_matrix\n",
        "\n",
        "    image_embeddings = None\n",
        "    if image_features:\n",
        "        image_feature_matrix = np.zeros((num_nodes, list(image_features.values())[0].shape[0]))\n",
        "        for node_id, embedding in image_features.items():\n",
        "            image_feature_matrix[int(node_id)] = embedding\n",
        "        image_embeddings = image_feature_matrix\n",
        "\n",
        "    return edge_index, labels, train_idx, valid_idx, test_idx, text_embeddings, image_embeddings, label_to_idx\n",
        "\n",
        "def train_gnn(kg_explorer, epochs=30, lr=0.001, hidden_dim=128, batch_size=512):\n",
        "    \"\"\"Train a Graph Neural Network for node classification\"\"\"\n",
        "    print(\"Starting GNN training...\")\n",
        "    text_features, image_features = extract_features(kg_explorer)\n",
        "    edge_index, labels, train_idx, valid_idx, test_idx, text_embeddings, image_embeddings, label_to_idx = \\\n",
        "        prepare_graph_data(kg_explorer, text_features, image_features)\n",
        "\n",
        "    num_nodes = len(kg_explorer.e2i)\n",
        "    num_classes = len(set(labels.numpy()))\n",
        "    model = MultimodalGNN(num_nodes, hidden_dim, num_classes, text_embeddings, image_embeddings).to(kg_explorer.device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "\n",
        "    edge_index = edge_index.to(kg_explorer.device)\n",
        "    labels = labels.to(kg_explorer.device)\n",
        "    train_idx = train_idx.to(kg_explorer.device)\n",
        "    valid_idx = valid_idx.to(kg_explorer.device)\n",
        "    test_idx = test_idx.to(kg_explorer.device)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    best_model = None\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(edge_index)\n",
        "        loss = F.cross_entropy(out[train_idx], labels[train_idx])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = model(edge_index)\n",
        "            pred = out.argmax(dim=1)\n",
        "            train_acc = (pred[train_idx] == labels[train_idx]).float().mean().item()\n",
        "            val_acc = (pred[valid_idx] == labels[valid_idx]).float().mean().item()\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_model = copy.deepcopy(model)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    best_model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = best_model(edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        test_acc = (pred[test_idx] == labels[test_idx]).float().mean().item()\n",
        "        test_f1 = f1_score(labels[test_idx].cpu().numpy(), pred[test_idx].cpu().numpy(), average='weighted')\n",
        "        target_names = [f\"Class {i}\" for i in range(num_classes)]\n",
        "        class_report = classification_report(labels[test_idx].cpu().numpy(), pred[test_idx].cpu().numpy(), target_names=target_names, output_dict=True)\n",
        "    print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Test F1 Score (weighted): {test_f1:.4f}\")\n",
        "\n",
        "    visualize_results(kg_explorer, best_model, edge_index, labels, test_idx, label_to_idx)\n",
        "    return best_model, test_acc\n",
        "\n",
        "def visualize_results(kg_explorer, model, edge_index, labels, test_idx, label_to_idx):\n",
        "    \"\"\"Visualize the results of the GNN model\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = model.node_emb.weight\n",
        "        if model.has_text:\n",
        "            text_proj = model.text_projection(model.text_embeddings)\n",
        "            x[model.text_nodes] = x[model.text_nodes] + text_proj\n",
        "        if model.has_image:\n",
        "            image_proj = model.image_projection(model.image_embeddings)\n",
        "            x[model.image_nodes] = x[model.image_nodes] + image_proj\n",
        "        x = F.relu(model.conv1(x, edge_index))\n",
        "        x = F.relu(model.conv2(x, edge_index))\n",
        "        embeddings = x.cpu().numpy()\n",
        "\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    test_embeddings = embeddings[test_idx.cpu().numpy()]\n",
        "    test_labels = labels[test_idx].cpu().numpy()\n",
        "    reduced_embeddings = tsne.fit_transform(test_embeddings)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    idx_to_label = {v: k for k, v in label_to_idx.items()}\n",
        "    unique_classes = sorted(set(test_labels))\n",
        "    for class_idx in unique_classes:\n",
        "        mask = test_labels == class_idx\n",
        "        plt.scatter(reduced_embeddings[mask, 0], reduced_embeddings[mask, 1], label=f\"{idx_to_label.get(class_idx, f'Class {class_idx}')}\", alpha=0.7)\n",
        "    plt.title(\"t-SNE Visualization of Node Embeddings\", fontsize=16)\n",
        "    plt.xlabel(\"Dimension 1\", fontsize=12)\n",
        "    plt.ylabel(\"Dimension 2\", fontsize=12)\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('node_embeddings_tsne.png', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    conf_matrix = confusion_matrix(test_labels, model(edge_index).argmax(dim=1).cpu().numpy()[test_idx.cpu().numpy()])\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(\"Confusion Matrix\", fontsize=16)\n",
        "    plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "    plt.ylabel(\"True Label\", fontsize=12)\n",
        "    plt.savefig('confusion_matrix.png', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "def jointly_finetune(kg_explorer, gnn_model, edge_index, labels, train_idx, valid_idx, test_idx, epochs=5):\n",
        "    \"\"\"Jointly fine-tune the CLIP and BERT models with GNN\"\"\"\n",
        "    print(\"Starting joint fine-tuning of pre-trained models...\")\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    bert_model = BertModel.from_pretrained('bert-base-uncased').to(kg_explorer.device)\n",
        "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(kg_explorer.device)\n",
        "\n",
        "    class JointModel(nn.Module):\n",
        "        def _init_(self, gnn_model, bert_model, clip_model):\n",
        "            super(JointModel, self)._init_()\n",
        "            self.gnn = gnn_model\n",
        "            self.bert = bert_model\n",
        "            self.clip = clip_model\n",
        "\n",
        "        def forward(self, edge_index, text_inputs=None, image_inputs=None):\n",
        "            return self.gnn(edge_index)\n",
        "\n",
        "    joint_model = JointModel(gnn_model, bert_model, clip_model).to(kg_explorer.device)\n",
        "    for param in bert_model.embeddings.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in clip_model.vision_model.embeddings.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    optimizer = torch.optim.Adam([\n",
        "        {'params': gnn_model.parameters(), 'lr': 0.001},\n",
        "        {'params': bert_model.encoder.parameters(), 'lr': 0.00003},\n",
        "        {'params': clip_model.vision_model.encoder.parameters(), 'lr': 0.00003}\n",
        "    ])\n",
        "\n",
        "    edge_index = edge_index.to(kg_explorer.device)\n",
        "    labels = labels.to(kg_explorer.device)\n",
        "    train_idx = train_idx.to(kg_explorer.device)\n",
        "    valid_idx = valid_idx.to(kg_explorer.device)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    best_model = None\n",
        "    for epoch in range(epochs):\n",
        "        joint_model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = joint_model(edge_index)\n",
        "        loss = F.cross_entropy(out[train_idx], labels[train_idx])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        joint_model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = joint_model(edge_index)\n",
        "            pred = out.argmax(dim=1)\n",
        "            train_acc = (pred[train_idx] == labels[train_idx]).float().mean().item()\n",
        "            val_acc = (pred[valid_idx] == labels[valid_idx]).float().mean().item()\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_model = copy.deepcopy(joint_model)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    best_model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = best_model(edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        test_acc = (pred[test_idx] == labels[test_idx]).float().mean().item()\n",
        "    print(f\"\\nJoint Fine-tuning Test Accuracy: {test_acc:.4f}\")\n",
        "    return best_model\n",
        "\n",
        "def main():\n",
        "    data_path = \"./dmg777k\"\n",
        "    explorer = KGBenchExplorer(data_path)\n",
        "\n",
        "    print(\"=== Running Exploratory Data Analysis ===\")\n",
        "    explorer.run_complete_eda()\n",
        "\n",
        "    print(\"\\n=== Training Multimodal GNN ===\")\n",
        "    gnn_model, test_accuracy = train_gnn(explorer, epochs=10)\n",
        "\n",
        "    if input(\"\\nDo you want to run the BONUS joint fine-tuning? (y/n): \").lower() == 'y':\n",
        "        print(\"\\n=== Running Bonus Task: Joint Fine-tuning ===\")\n",
        "        text_features, image_features = extract_features(explorer)\n",
        "        edge_index, labels, train_idx, valid_idx, test_idx, text_embeddings, image_embeddings, label_to_idx = \\\n",
        "            prepare_graph_data(explorer, text_features, image_features)\n",
        "        joint_model = jointly_finetune(explorer, gnn_model, edge_index, labels, train_idx, valid_idx, test_idx, epochs=2)\n",
        "        print(\"Joint fine-tuning completed!\")\n",
        "\n",
        "    print(\"\\n=== Task Completed ===\")\n",
        "\n",
        "if _name_ == \"_main_\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}